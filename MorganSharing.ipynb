{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNy3PUJcH3YZ/rbQD5C6U7x"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J7Y-Vs3ETlrS"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import datetime as dt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# initial read of input csv files with converters\n",
        "\n",
        "rawExcess = pd.read_csv('/content/Copy of MG-M0076-Excess_Inventory_-_18_Week.csv', thousands = ',')\n",
        "rawDemand = pd.read_csv('/content/MG-O0717-DRP_Branch_Reference.csv', thousands = ',')\n",
        "\n",
        "# data cleaning processes\n",
        "\n",
        "# trim column names\n",
        "\n",
        "rawExcess.columns = rawExcess.columns.str.strip()\n",
        "rawDemand.columns = rawDemand.columns.str.strip()\n",
        "\n",
        "# remove dollar signs\n",
        "\n",
        "rawExcess['Standard Cost'] = rawExcess['Standard Cost'].str.replace(',', '').str.replace('$', '')\n",
        "\n",
        "# NaN to 0 for later inequalities\n",
        "\n",
        "rawExcess = rawExcess.fillna(0)\n",
        "rawDemand = rawDemand.fillna(0)\n",
        "\n",
        "\n",
        "# convert data types\n",
        "\n",
        "\n",
        "# for excess\n",
        "rawExcess['Supply - Open P.O. Qty'] = pd.to_numeric(rawExcess['Supply - Open P.O. Qty'],errors = 'raise')\n",
        "rawExcess['Supply_18 Wk'] = pd.to_numeric(rawExcess['Supply_18 Wk'],errors = 'raise')\n",
        "rawExcess['Demand_18 Wk'] = pd.to_numeric(rawExcess['Demand_18 Wk'],errors = 'raise')\n",
        "rawExcess['Excess Qty_18 Wk'] = pd.to_numeric(rawExcess['Excess Qty_18 Wk'],errors = 'raise')\n",
        "rawExcess['Standard Cost'] = pd.to_numeric(rawExcess['Standard Cost'],errors = 'raise')\n",
        "\n",
        "# for demand\n",
        "\n",
        "rawDemand['Request Date'] = pd.to_datetime(rawDemand['Request Date'],errors='ignore')\n",
        "rawDemand['Trans QTY'] = np.floor(rawDemand['Trans QTY']).astype(int)\n",
        "rawDemand['Header Branch'] = np.floor(rawDemand['Header Branch']).astype(int)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sGTrUQf9U-zh",
        "outputId": "d31f679f-2176-4109-9cf5-fc2b4f911d12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-b7206002fddc>:15: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
            "  rawExcess['Standard Cost'] = rawExcess['Standard Cost'].str.replace(',', '').str.replace('$', '')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# roll up values in Excess to reduce unneccesary columns/observations\n",
        "\n",
        "# define what you want to count as 'True Excess' \n",
        "\n",
        "columns = ['Plan Fmly','Supply - Open P.O. Qty','Demand_18 Wk','Excess Qty_18 Wk']\n",
        "conditions = [(rawExcess['Plan Fmly'] != 'M23') & (rawExcess['Supply - Open P.O. Qty']<= 0) & (rawExcess['Demand_18 Wk'] <= 0) & (rawExcess['Excess Qty_18 Wk'] > 0)]\n",
        "\n",
        "# remove CSM, DCL and PNT\n",
        "\n",
        "rawExcess = rawExcess[~rawExcess['Part'].str.contains('CSM')]\n",
        "rawExcess = rawExcess[~rawExcess['Part Description'].str.contains('DCL')]\n",
        "rawExcess = rawExcess[~rawExcess['Part Description'].str.contains('PNT')]\n",
        "\n",
        "# use boolean indexing to filter the Dataframe to only include 'True Excess'\n",
        "\n",
        "rawExcess = rawExcess[conditions[0]]\n",
        "\n",
        "# finally remove unneccesary columns and add columns to hold later calculations\n",
        "\n",
        "excess = rawExcess[['Branch Plant','Part','Part Description','Standard Cost','Excess Qty_18 Wk']]"
      ],
      "metadata": {
        "id": "fgOXR1oqZcS8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34761b0b-9fb4-4819-ac95-298c7d805333"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-0b2f929a06b7>:16: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "  rawExcess = rawExcess[conditions[0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create new array for each individual plant's excess\n",
        "\n",
        "# Group the 'excess' DataFrame by the 'Branch Plant' column and create a dictionary of DataFrames\n",
        "df_dict = dict(tuple(excess.groupby('Branch Plant')))\n",
        "\n",
        "excess1501 = df_dict[1501]\n",
        "excess1502 = df_dict[1502]\n",
        "excess1503 = df_dict[1503]\n",
        "excess1504 = df_dict[1504]\n",
        "excess1505 = df_dict[1505]\n",
        "excess1506 = df_dict[1506]\n",
        "excess1507 = df_dict[1507]\n",
        "excess1508 = df_dict[1508]\n",
        "excess1509 = df_dict[1509]\n",
        "excess1510 = df_dict[1510]\n",
        "excess1511 = df_dict[1511]\n",
        "excess1512 = df_dict[1512]\n",
        "excess1601 = df_dict[1601]\n",
        "excess1602 = df_dict[1602]"
      ],
      "metadata": {
        "id": "yTsHVTQoXUrR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove unneccesary columns from demand\n",
        "\n",
        "demand = rawDemand[['2nd Item Number','Header Branch','Trans QTY','Request Date']]"
      ],
      "metadata": {
        "id": "6tE9InLm1hUX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# group by 'Header Branch' and '2nd Item Number', and aggregate using sum and max functions\n",
        "\n",
        "demand_sum = demand.groupby(['Header Branch', '2nd Item Number']).agg({'Trans QTY': 'sum', 'Request Date': 'min'}).reset_index()"
      ],
      "metadata": {
        "id": "i8RZsmf4fgNn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# separate all demand by branch plant\n",
        "\n",
        "# group by 'Header Branch'\n",
        "\n",
        "groups = demand_sum.groupby('Header Branch')\n",
        "\n",
        "# create a new DataFrame for each group\n",
        "\n",
        "branchdemands = []\n",
        "for name, group in groups:\n",
        "    branchdemands.append(group.copy())"
      ],
      "metadata": {
        "id": "-xIbFrSrhc-z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# separate demand into individual data frames\n",
        "\n",
        "demand1501 = branchdemands[1]\n",
        "demand1502 = branchdemands[2]\n",
        "demand1503 = branchdemands[3]\n",
        "demand1504 = branchdemands[4]\n",
        "demand1505 = branchdemands[5]\n",
        "demand1506 = branchdemands[6]\n",
        "demand1507 = branchdemands[7]\n",
        "demand1508 = branchdemands[8]\n",
        "demand1509 = branchdemands[9]\n",
        "demand1510 = branchdemands[10]\n",
        "demand1511 = branchdemands[11]\n",
        "demand1512 = branchdemands[12]\n",
        "demand1601 = branchdemands[13]\n",
        "demand1602 = branchdemands[14]"
      ],
      "metadata": {
        "id": "qYUVuK_yhOos"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create ordered lists for sharing\n",
        "\n",
        "order_1501 = [demand1509, demand1506, demand1511, demand1512, demand1601, demand1602, demand1503, demand1502, demand1508, demand1505, demand1504, demand1507, demand1510]\n",
        "order_1502 = [demand1512, demand1601, demand1602, demand1503, demand1506, demand1509, demand1501, demand1505, demand1511, demand1508, demand1504, demand1507, demand1510]\n",
        "order_1503 = [demand1508, demand1512, demand1506, demand1502, demand1509, demand1501, demand1505, demand1601, demand1602, demand1511, demand1504, demand1507, demand1510]\n",
        "order_1504 = [demand1507, demand1510, demand1505, demand1502, demand1503, demand1512, demand1508, demand1601, demand1602, demand1506, demand1509, demand1501, demand1511]\n",
        "order_1505 = [demand1503, demand1502, demand1508, demand1512, demand1504, demand1601, demand1507, demand1602, demand1506, demand1509, demand1501, demand1511, demand1510]\n",
        "order_1506 = [demand1509, demand1501, demand1511, demand1512, demand1601, demand1602, demand1503, demand1502, demand1508, demand1505, demand1504, demand1507, demand1510]\n",
        "order_1507 = [demand1504, demand1510, demand1505, demand1502, demand1503, demand1512, demand1601, demand1602, demand1508, demand1506, demand1509, demand1501, demand1511]\n",
        "order_1508 = [demand1503, demand1512, demand1506, demand1509, demand1501, demand1505, demand1502, demand1601, demand1511, demand1602, demand1504, demand1507, demand1510]\n",
        "order_1509 = [demand1506, demand1501, demand1511, demand1512, demand1601, demand1602, demand1503, demand1502, demand1508, demand1505, demand1504, demand1507, demand1510]\n",
        "order_1510 = [demand1507, demand1504, demand1502, demand1505, demand1512, demand1602, demand1601, demand1503, demand1506, demand1509, demand1501, demand1511, demand1508]\n",
        "order_1511 = [demand1501, demand1509, demand1506, demand1601, demand1602, demand1512, demand1503, demand1502, demand1508, demand1505, demand1504, demand1507, demand1510]\n",
        "order_1512 = [demand1601, demand1602, demand1506, demand1509, demand1501, demand1502, demand1503, demand1511, demand1508, demand1505, demand1504, demand1507, demand1510]\n",
        "order_1601 = [demand1602, demand1512, demand1509, demand1506, demand1501, demand1511, demand1502, demand1503, demand1508, demand1505, demand1504, demand1510, demand1507]\n",
        "order_1602 = [demand1601, demand1512, demand1509, demand1506, demand1501, demand1511, demand1502, demand1503, demand1508, demand1505, demand1504, demand1510, demand1507]\n",
        "\n",
        "order_1501 = order_1501[::-1]\n",
        "order_1502 = order_1502[::-1]\n",
        "order_1503 = order_1503[::-1]\n",
        "order_1504 = order_1504[::-1]\n",
        "order_1505 = order_1505[::-1]\n",
        "order_1506 = order_1506[::-1]\n",
        "order_1507 = order_1507[::-1]\n",
        "order_1508 = order_1508[::-1]\n",
        "order_1509 = order_1509[::-1]\n",
        "order_1510 = order_1510[::-1]\n",
        "order_1511 = order_1511[::-1]\n",
        "order_1512 = order_1512[::-1]\n",
        "order_1601 = order_1601[::-1]\n",
        "order_1602 = order_1602[::-1]"
      ],
      "metadata": {
        "id": "HohJRTbis-z_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create column order\n",
        "cols = ['Branch Plant','Part','Part Description','Standard Cost','Excess Qty_18 Wk','Header Branch','Trans QTY','Request Date']"
      ],
      "metadata": {
        "id": "3telFv3JAgKQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create loop that creates individual data frames for each part match from excess data frames to demand data frames\n",
        "\n",
        "matches1501 = pd.DataFrame()\n",
        "\n",
        "\n",
        "for df in order_1501:\n",
        "    merged = pd.merge(excess1501, df, left_on='Part', right_on='2nd Item Number', how='inner')\n",
        "    \n",
        "    # Add the merged data to the results data frame\n",
        "    matches1501 = pd.concat([merged, matches1501])\n",
        "# rearrange dataframe for easier understanding\n",
        "\n",
        "matches1501 = matches1501[cols]\n",
        "\n",
        "# run again for the next plant\n",
        "\n",
        "matches1502 = pd.DataFrame()\n",
        "\n",
        "\n",
        "for df in order_1502:\n",
        "    merged = pd.merge(excess1502, df, left_on='Part', right_on='2nd Item Number', how='inner')\n",
        "    \n",
        "    # Add the merged data to the results data frame\n",
        "    matches1502 = pd.concat([merged, matches1502])\n",
        "# rearrange dataframe for easier understanding\n",
        "\n",
        "matches1502 = matches1502[cols]\n",
        "\n",
        "# run again for the next plant\n",
        "\n",
        "matches1503 = pd.DataFrame()\n",
        "\n",
        "\n",
        "for df in order_1503:\n",
        "    merged = pd.merge(excess1503, df, left_on='Part', right_on='2nd Item Number', how='inner')\n",
        "    \n",
        "    # Add the merged data to the results data frame\n",
        "    matches1503 = pd.concat([merged, matches1503])\n",
        "# rearrange dataframe for easier understanding\n",
        "\n",
        "matches1503 = matches1503[cols]\n",
        "\n",
        "# run again for the next plant\n",
        "\n",
        "matches1504 = pd.DataFrame()\n",
        "\n",
        "\n",
        "for df in order_1504:\n",
        "    merged = pd.merge(excess1504, df, left_on='Part', right_on='2nd Item Number', how='inner')\n",
        "    \n",
        "    # Add the merged data to the results data frame\n",
        "    matches1504 = pd.concat([merged, matches1504])\n",
        "# rearrange dataframe for easier understanding\n",
        "\n",
        "matches1504 = matches1504[cols]\n",
        "\n",
        "# run again for the next plant\n",
        "\n",
        "matches1505 = pd.DataFrame()\n",
        "\n",
        "\n",
        "for df in order_1505:\n",
        "    merged = pd.merge(excess1505, df, left_on='Part', right_on='2nd Item Number', how='inner')\n",
        "    \n",
        "    # Add the merged data to the results data frame\n",
        "    matches1505 = pd.concat([merged, matches1505])\n",
        "# rearrange dataframe for easier understanding\n",
        "\n",
        "matches1505 = matches1505[cols]\n",
        "\n",
        "# run again for the next plant\n",
        "\n",
        "matches1506 = pd.DataFrame()\n",
        "\n",
        "\n",
        "for df in order_1506:\n",
        "    merged = pd.merge(excess1506, df, left_on='Part', right_on='2nd Item Number', how='inner')\n",
        "    \n",
        "    # Add the merged data to the results data frame\n",
        "    matches1506 = pd.concat([merged, matches1506])\n",
        "# rearrange dataframe for easier understanding\n",
        "\n",
        "matches1506 = matches1506[cols]\n",
        "\n",
        "# run again for the next plant\n",
        "\n",
        "matches1507 = pd.DataFrame()\n",
        "\n",
        "\n",
        "for df in order_1507:\n",
        "    merged = pd.merge(excess1507, df, left_on='Part', right_on='2nd Item Number', how='inner')\n",
        "    \n",
        "    # Add the merged data to the results data frame\n",
        "    matches1507 = pd.concat([merged, matches1507])\n",
        "# rearrange dataframe for easier understanding\n",
        "\n",
        "matches1507 = matches1507[cols]\n",
        "\n",
        "# run again for the next plant\n",
        "\n",
        "matches1508 = pd.DataFrame()\n",
        "\n",
        "\n",
        "for df in order_1508:\n",
        "    merged = pd.merge(excess1508, df, left_on='Part', right_on='2nd Item Number', how='inner')\n",
        "    \n",
        "    # Add the merged data to the results data frame\n",
        "    matches1508 = pd.concat([merged, matches1508])\n",
        "# rearrange dataframe for easier understanding\n",
        "\n",
        "matches1508 = matches1508[cols]\n",
        "\n",
        "# run again for the next plant\n",
        "\n",
        "matches1509 = pd.DataFrame()\n",
        "\n",
        "\n",
        "for df in order_1509:\n",
        "    merged = pd.merge(excess1509, df, left_on='Part', right_on='2nd Item Number', how='inner')\n",
        "    \n",
        "    # Add the merged data to the results data frame\n",
        "    matches1509 = pd.concat([merged, matches1509])\n",
        "# rearrange dataframe for easier understanding\n",
        "\n",
        "matches1509 = matches1509[cols]\n",
        "\n",
        "# run again for the next plant\n",
        "\n",
        "matches1510 = pd.DataFrame()\n",
        "\n",
        "\n",
        "for df in order_1510:\n",
        "    merged = pd.merge(excess1510, df, left_on='Part', right_on='2nd Item Number', how='inner')\n",
        "    \n",
        "    # Add the merged data to the results data frame\n",
        "    matches1510 = pd.concat([merged, matches1510])\n",
        "# rearrange dataframe for easier understanding\n",
        "\n",
        "matches1510  = matches1510[cols]\n",
        "\n",
        "# run again for the next plant\n",
        "\n",
        "matches1511 = pd.DataFrame()\n",
        "\n",
        "\n",
        "for df in order_1511:\n",
        "    merged = pd.merge(excess1511, df, left_on='Part', right_on='2nd Item Number', how='inner')\n",
        "    \n",
        "    # Add the merged data to the results data frame\n",
        "    matches1511 = pd.concat([merged, matches1511])\n",
        "# rearrange dataframe for easier understanding\n",
        "\n",
        "matches1511 = matches1511[cols]\n",
        "\n",
        "# run again for the next plant\n",
        "\n",
        "matches1512 = pd.DataFrame()\n",
        "\n",
        "\n",
        "for df in order_1512:\n",
        "    merged = pd.merge(excess1512, df, left_on='Part', right_on='2nd Item Number', how='inner')\n",
        "    \n",
        "    # Add the merged data to the results data frame\n",
        "    matches1512 = pd.concat([merged, matches1512])\n",
        "# rearrange dataframe for easier understanding\n",
        "\n",
        "\n",
        "matches1512 = matches1512[cols]\n",
        "\n",
        "# run again for the next plant\n",
        "\n",
        "matches1601 = pd.DataFrame()\n",
        "\n",
        "\n",
        "for df in order_1601:\n",
        "    merged = pd.merge(excess1601, df, left_on='Part', right_on='2nd Item Number', how='inner')\n",
        "    \n",
        "    # Add the merged data to the results data frame\n",
        "    matches1601 = pd.concat([merged, matches1601])\n",
        "# rearrange dataframe for easier understanding\n",
        "\n",
        "matches1601 = matches1601[cols]\n",
        "\n",
        "# run again for the next plant\n",
        "\n",
        "matches1602 = pd.DataFrame()\n",
        "\n",
        "\n",
        "for df in order_1602:\n",
        "    merged = pd.merge(excess1602, df, left_on='Part', right_on='2nd Item Number', how='inner')\n",
        "    \n",
        "    # Add the merged data to the results data frame\n",
        "    matches1602 = pd.concat([merged, matches1602])\n",
        "# rearrange dataframe for easier understanding\n",
        "\n",
        "matches1602  = matches1602[cols]\n",
        "\n",
        "matches = [matches1501,matches1502,matches1503,matches1504,matches1505,matches1506,matches1507,matches1508,matches1509,matches1510,matches1511,matches1512,matches1601,matches1602]"
      ],
      "metadata": {
        "id": "g8iL_GBr5z4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loop over each unique part in the 'Part' column\n",
        "\n",
        "#creating 1501s sharing report\n",
        "\n",
        "combined_df = pd.DataFrame()\n",
        "\n",
        "for part in matches1501['Part'].unique():\n",
        "    # Filter the data frame for the current part and matches1501\n",
        "    part_df = matches1501[(matches1501['Part'] == part)]\n",
        "    # Define the new columns\n",
        "    taken_col = []\n",
        "    leftover_col = []\n",
        "    part_df = part_df.reset_index(drop=True)\n",
        "    \n",
        "    # Loop over each row in the filtered data frame\n",
        "    for i, row in part_df.iterrows():\n",
        "        if i == 0:\n",
        "            # For the first row, calculate the 'taken' and 'leftover' values\n",
        "            if row['Excess Qty_18 Wk'] > row['Trans QTY']:\n",
        "                taken = row['Trans QTY']\n",
        "            else:\n",
        "                taken = row['Excess Qty_18 Wk']\n",
        "            taken_col.append(taken)\n",
        "            leftover = row['Excess Qty_18 Wk'] - taken\n",
        "            leftover_col.append(leftover)\n",
        "        else:\n",
        "            # For all following rows, calculate the 'taken' and 'leftover' values based on the previous row's 'leftover' value\n",
        "            prev_leftover = leftover_col[-1]\n",
        "            if prev_leftover > row['Trans QTY']:\n",
        "                taken = row['Trans QTY']\n",
        "            else:\n",
        "                taken = prev_leftover\n",
        "            taken_col.append(taken)\n",
        "            leftover = prev_leftover - taken\n",
        "            leftover_col.append(leftover)\n",
        "    # Add the new columns to the filtered data frame\n",
        "    part_df['taken'] = taken_col\n",
        "    part_df['leftover'] = leftover_col\n",
        "    combined_df = pd.concat([combined_df, part_df], ignore_index=True)\n",
        "df1501 = combined_df[combined_df['taken']>0]\n",
        "\n",
        "#creating 1502s sharing report\n",
        "\n",
        "combined_df = pd.DataFrame()\n",
        "\n",
        "for part in matches1502['Part'].unique():\n",
        "    # Filter the data frame for the current part and matches1501\n",
        "    part_df = matches1502[(matches1502['Part'] == part)]\n",
        "    # Define the new columns\n",
        "    taken_col = []\n",
        "    leftover_col = []\n",
        "    part_df = part_df.reset_index(drop=True)\n",
        "    \n",
        "    # Loop over each row in the filtered data frame\n",
        "    for i, row in part_df.iterrows():\n",
        "        if i == 0:\n",
        "            # For the first row, calculate the 'taken' and 'leftover' values\n",
        "            if row['Excess Qty_18 Wk'] > row['Trans QTY']:\n",
        "                taken = row['Trans QTY']\n",
        "            else:\n",
        "                taken = row['Excess Qty_18 Wk']\n",
        "            taken_col.append(taken)\n",
        "            leftover = row['Excess Qty_18 Wk'] - taken\n",
        "            leftover_col.append(leftover)\n",
        "        else:\n",
        "            # For all following rows, calculate the 'taken' and 'leftover' values based on the previous row's 'leftover' value\n",
        "            prev_leftover = leftover_col[-1]\n",
        "            if prev_leftover > row['Trans QTY']:\n",
        "                taken = row['Trans QTY']\n",
        "            else:\n",
        "                taken = prev_leftover\n",
        "            taken_col.append(taken)\n",
        "            leftover = prev_leftover - taken\n",
        "            leftover_col.append(leftover)\n",
        "    # Add the new columns to the filtered data frame\n",
        "    part_df['taken'] = taken_col\n",
        "    part_df['leftover'] = leftover_col\n",
        "    combined_df = pd.concat([combined_df, part_df], ignore_index=True)\n",
        "df1502 = combined_df[combined_df['taken']>0]\n",
        "\n",
        "#creating 1503s sharing report\n",
        "\n",
        "combined_df = pd.DataFrame()\n",
        "\n",
        "for part in matches1503['Part'].unique():\n",
        "    # Filter the data frame for the current part and matches1501\n",
        "    part_df = matches1503[(matches1503['Part'] == part)]\n",
        "    # Define the new columns\n",
        "    taken_col = []\n",
        "    leftover_col = []\n",
        "    part_df = part_df.reset_index(drop=True)\n",
        "    \n",
        "    # Loop over each row in the filtered data frame\n",
        "    for i, row in part_df.iterrows():\n",
        "        if i == 0:\n",
        "            # For the first row, calculate the 'taken' and 'leftover' values\n",
        "            if row['Excess Qty_18 Wk'] > row['Trans QTY']:\n",
        "                taken = row['Trans QTY']\n",
        "            else:\n",
        "                taken = row['Excess Qty_18 Wk']\n",
        "            taken_col.append(taken)\n",
        "            leftover = row['Excess Qty_18 Wk'] - taken\n",
        "            leftover_col.append(leftover)\n",
        "        else:\n",
        "            # For all following rows, calculate the 'taken' and 'leftover' values based on the previous row's 'leftover' value\n",
        "            prev_leftover = leftover_col[-1]\n",
        "            if prev_leftover > row['Trans QTY']:\n",
        "                taken = row['Trans QTY']\n",
        "            else:\n",
        "                taken = prev_leftover\n",
        "            taken_col.append(taken)\n",
        "            leftover = prev_leftover - taken\n",
        "            leftover_col.append(leftover)\n",
        "    # Add the new columns to the filtered data frame\n",
        "    part_df['taken'] = taken_col\n",
        "    part_df['leftover'] = leftover_col\n",
        "    combined_df = pd.concat([combined_df, part_df], ignore_index=True)\n",
        "df1503 = combined_df[combined_df['taken']>0]\n",
        "\n",
        "#creating 1504s sharing report\n",
        "\n",
        "combined_df = pd.DataFrame()\n",
        "\n",
        "for part in matches1504['Part'].unique():\n",
        "    # Filter the data frame for the current part and matches1501\n",
        "    part_df = matches1504[(matches1504['Part'] == part)]\n",
        "    # Define the new columns\n",
        "    taken_col = []\n",
        "    leftover_col = []\n",
        "    part_df = part_df.reset_index(drop=True)\n",
        "    \n",
        "    # Loop over each row in the filtered data frame\n",
        "    for i, row in part_df.iterrows():\n",
        "        if i == 0:\n",
        "            # For the first row, calculate the 'taken' and 'leftover' values\n",
        "            if row['Excess Qty_18 Wk'] > row['Trans QTY']:\n",
        "                taken = row['Trans QTY']\n",
        "            else:\n",
        "                taken = row['Excess Qty_18 Wk']\n",
        "            taken_col.append(taken)\n",
        "            leftover = row['Excess Qty_18 Wk'] - taken\n",
        "            leftover_col.append(leftover)\n",
        "        else:\n",
        "            # For all following rows, calculate the 'taken' and 'leftover' values based on the previous row's 'leftover' value\n",
        "            prev_leftover = leftover_col[-1]\n",
        "            if prev_leftover > row['Trans QTY']:\n",
        "                taken = row['Trans QTY']\n",
        "            else:\n",
        "                taken = prev_leftover\n",
        "            taken_col.append(taken)\n",
        "            leftover = prev_leftover - taken\n",
        "            leftover_col.append(leftover)\n",
        "    # Add the new columns to the filtered data frame\n",
        "    part_df['taken'] = taken_col\n",
        "    part_df['leftover'] = leftover_col\n",
        "    combined_df = pd.concat([combined_df, part_df], ignore_index=True)\n",
        "df1504 = combined_df[combined_df['taken']>0]\n",
        "\n",
        "#creating 1505s sharing report\n",
        "\n",
        "combined_df = pd.DataFrame()\n",
        "\n",
        "for part in matches1505['Part'].unique():\n",
        "    # Filter the data frame for the current part and matches1501\n",
        "    part_df = matches1505[(matches1505['Part'] == part)]\n",
        "    # Define the new columns\n",
        "    taken_col = []\n",
        "    leftover_col = []\n",
        "    part_df = part_df.reset_index(drop=True)\n",
        "    \n",
        "    # Loop over each row in the filtered data frame\n",
        "    for i, row in part_df.iterrows():\n",
        "        if i == 0:\n",
        "            # For the first row, calculate the 'taken' and 'leftover' values\n",
        "            if row['Excess Qty_18 Wk'] > row['Trans QTY']:\n",
        "                taken = row['Trans QTY']\n",
        "            else:\n",
        "                taken = row['Excess Qty_18 Wk']\n",
        "            taken_col.append(taken)\n",
        "            leftover = row['Excess Qty_18 Wk'] - taken\n",
        "            leftover_col.append(leftover)\n",
        "        else:\n",
        "            # For all following rows, calculate the 'taken' and 'leftover' values based on the previous row's 'leftover' value\n",
        "            prev_leftover = leftover_col[-1]\n",
        "            if prev_leftover > row['Trans QTY']:\n",
        "                taken = row['Trans QTY']\n",
        "            else:\n",
        "                taken = prev_leftover\n",
        "            taken_col.append(taken)\n",
        "            leftover = prev_leftover - taken\n",
        "            leftover_col.append(leftover)\n",
        "    # Add the new columns to the filtered data frame\n",
        "    part_df['taken'] = taken_col\n",
        "    part_df['leftover'] = leftover_col\n",
        "    combined_df = pd.concat([combined_df, part_df], ignore_index=True)\n",
        "df1505 = combined_df[combined_df['taken']>0]\n",
        "\n",
        "#creating 1506s sharing report\n",
        "\n",
        "combined_df = pd.DataFrame()\n",
        "\n",
        "for part in matches1506['Part'].unique():\n",
        "    # Filter the data frame for the current part and matches1501\n",
        "    part_df = matches1506[(matches1506['Part'] == part)]\n",
        "    # Define the new columns\n",
        "    taken_col = []\n",
        "    leftover_col = []\n",
        "    part_df = part_df.reset_index(drop=True)\n",
        "    \n",
        "    # Loop over each row in the filtered data frame\n",
        "    for i, row in part_df.iterrows():\n",
        "        if i == 0:\n",
        "            # For the first row, calculate the 'taken' and 'leftover' values\n",
        "            if row['Excess Qty_18 Wk'] > row['Trans QTY']:\n",
        "                taken = row['Trans QTY']\n",
        "            else:\n",
        "                taken = row['Excess Qty_18 Wk']\n",
        "            taken_col.append(taken)\n",
        "            leftover = row['Excess Qty_18 Wk'] - taken\n",
        "            leftover_col.append(leftover)\n",
        "        else:\n",
        "            # For all following rows, calculate the 'taken' and 'leftover' values based on the previous row's 'leftover' value\n",
        "            prev_leftover = leftover_col[-1]\n",
        "            if prev_leftover > row['Trans QTY']:\n",
        "                taken = row['Trans QTY']\n",
        "            else:\n",
        "                taken = prev_leftover\n",
        "            taken_col.append(taken)\n",
        "            leftover = prev_leftover - taken\n",
        "            leftover_col.append(leftover)\n",
        "    # Add the new columns to the filtered data frame\n",
        "    part_df['taken'] = taken_col\n",
        "    part_df['leftover'] = leftover_col\n",
        "    combined_df = pd.concat([combined_df, part_df], ignore_index=True)\n",
        "df1506 = combined_df[combined_df['taken']>0]\n",
        "\n",
        "#creating 1507s sharing report\n",
        "\n",
        "combined_df = pd.DataFrame()\n",
        "\n",
        "for part in matches1507['Part'].unique():\n",
        "    # Filter the data frame for the current part and matches1501\n",
        "    part_df = matches1507[(matches1507['Part'] == part)]\n",
        "    # Define the new columns\n",
        "    taken_col = []\n",
        "    leftover_col = []\n",
        "    part_df = part_df.reset_index(drop=True)\n",
        "    \n",
        "    # Loop over each row in the filtered data frame\n",
        "    for i, row in part_df.iterrows():\n",
        "        if i == 0:\n",
        "            # For the first row, calculate the 'taken' and 'leftover' values\n",
        "            if row['Excess Qty_18 Wk'] > row['Trans QTY']:\n",
        "                taken = row['Trans QTY']\n",
        "            else:\n",
        "                taken = row['Excess Qty_18 Wk']\n",
        "            taken_col.append(taken)\n",
        "            leftover = row['Excess Qty_18 Wk'] - taken\n",
        "            leftover_col.append(leftover)\n",
        "        else:\n",
        "            # For all following rows, calculate the 'taken' and 'leftover' values based on the previous row's 'leftover' value\n",
        "            prev_leftover = leftover_col[-1]\n",
        "            if prev_leftover > row['Trans QTY']:\n",
        "                taken = row['Trans QTY']\n",
        "            else:\n",
        "                taken = prev_leftover\n",
        "            taken_col.append(taken)\n",
        "            leftover = prev_leftover - taken\n",
        "            leftover_col.append(leftover)\n",
        "    # Add the new columns to the filtered data frame\n",
        "    part_df['taken'] = taken_col\n",
        "    part_df['leftover'] = leftover_col\n",
        "    combined_df = pd.concat([combined_df, part_df], ignore_index=True)\n",
        "df1507 = combined_df[combined_df['taken']>0]\n",
        "\n",
        "#creating 1508s sharing report\n",
        "\n",
        "combined_df = pd.DataFrame()\n",
        "\n",
        "for part in matches1508['Part'].unique():\n",
        "    # Filter the data frame for the current part and matches1501\n",
        "    part_df = matches1508[(matches1508['Part'] == part)]\n",
        "    # Define the new columns\n",
        "    taken_col = []\n",
        "    leftover_col = []\n",
        "    part_df = part_df.reset_index(drop=True)\n",
        "    \n",
        "    # Loop over each row in the filtered data frame\n",
        "    for i, row in part_df.iterrows():\n",
        "        if i == 0:\n",
        "            # For the first row, calculate the 'taken' and 'leftover' values\n",
        "            if row['Excess Qty_18 Wk'] > row['Trans QTY']:\n",
        "                taken = row['Trans QTY']\n",
        "            else:\n",
        "                taken = row['Excess Qty_18 Wk']\n",
        "            taken_col.append(taken)\n",
        "            leftover = row['Excess Qty_18 Wk'] - taken\n",
        "            leftover_col.append(leftover)\n",
        "        else:\n",
        "            # For all following rows, calculate the 'taken' and 'leftover' values based on the previous row's 'leftover' value\n",
        "            prev_leftover = leftover_col[-1]\n",
        "            if prev_leftover > row['Trans QTY']:\n",
        "                taken = row['Trans QTY']\n",
        "            else:\n",
        "                taken = prev_leftover\n",
        "            taken_col.append(taken)\n",
        "            leftover = prev_leftover - taken\n",
        "            leftover_col.append(leftover)\n",
        "    # Add the new columns to the filtered data frame\n",
        "    part_df['taken'] = taken_col\n",
        "    part_df['leftover'] = leftover_col\n",
        "    combined_df = pd.concat([combined_df, part_df], ignore_index=True)\n",
        "df1508 = combined_df[combined_df['taken']>0]\n",
        "\n",
        "#creating 1509s sharing report\n",
        "\n",
        "combined_df = pd.DataFrame()\n",
        "\n",
        "for part in matches1509['Part'].unique():\n",
        "    # Filter the data frame for the current part and matches1501\n",
        "    part_df = matches1509[(matches1509['Part'] == part)]\n",
        "    # Define the new columns\n",
        "    taken_col = []\n",
        "    leftover_col = []\n",
        "    part_df = part_df.reset_index(drop=True)\n",
        "    \n",
        "    # Loop over each row in the filtered data frame\n",
        "    for i, row in part_df.iterrows():\n",
        "        if i == 0:\n",
        "            # For the first row, calculate the 'taken' and 'leftover' values\n",
        "            if row['Excess Qty_18 Wk'] > row['Trans QTY']:\n",
        "                taken = row['Trans QTY']\n",
        "            else:\n",
        "                taken = row['Excess Qty_18 Wk']\n",
        "            taken_col.append(taken)\n",
        "            leftover = row['Excess Qty_18 Wk'] - taken\n",
        "            leftover_col.append(leftover)\n",
        "        else:\n",
        "            # For all following rows, calculate the 'taken' and 'leftover' values based on the previous row's 'leftover' value\n",
        "            prev_leftover = leftover_col[-1]\n",
        "            if prev_leftover > row['Trans QTY']:\n",
        "                taken = row['Trans QTY']\n",
        "            else:\n",
        "                taken = prev_leftover\n",
        "            taken_col.append(taken)\n",
        "            leftover = prev_leftover - taken\n",
        "            leftover_col.append(leftover)\n",
        "    # Add the new columns to the filtered data frame\n",
        "    part_df['taken'] = taken_col\n",
        "    part_df['leftover'] = leftover_col\n",
        "    combined_df = pd.concat([combined_df, part_df], ignore_index=True)\n",
        "df1509 = combined_df[combined_df['taken']>0]\n",
        "\n",
        "#creating 1510s sharing report\n",
        "\n",
        "combined_df = pd.DataFrame()\n",
        "\n",
        "for part in matches1510['Part'].unique():\n",
        "    # Filter the data frame for the current part and matches1501\n",
        "    part_df = matches1510[(matches1510['Part'] == part)]\n",
        "    # Define the new columns\n",
        "    taken_col = []\n",
        "    leftover_col = []\n",
        "    part_df = part_df.reset_index(drop=True)\n",
        "    \n",
        "    # Loop over each row in the filtered data frame\n",
        "    for i, row in part_df.iterrows():\n",
        "        if i == 0:\n",
        "            # For the first row, calculate the 'taken' and 'leftover' values\n",
        "            if row['Excess Qty_18 Wk'] > row['Trans QTY']:\n",
        "                taken = row['Trans QTY']\n",
        "            else:\n",
        "                taken = row['Excess Qty_18 Wk']\n",
        "            taken_col.append(taken)\n",
        "            leftover = row['Excess Qty_18 Wk'] - taken\n",
        "            leftover_col.append(leftover)\n",
        "        else:\n",
        "            # For all following rows, calculate the 'taken' and 'leftover' values based on the previous row's 'leftover' value\n",
        "            prev_leftover = leftover_col[-1]\n",
        "            if prev_leftover > row['Trans QTY']:\n",
        "                taken = row['Trans QTY']\n",
        "            else:\n",
        "                taken = prev_leftover\n",
        "            taken_col.append(taken)\n",
        "            leftover = prev_leftover - taken\n",
        "            leftover_col.append(leftover)\n",
        "    # Add the new columns to the filtered data frame\n",
        "    part_df['taken'] = taken_col\n",
        "    part_df['leftover'] = leftover_col\n",
        "    combined_df = pd.concat([combined_df, part_df], ignore_index=True)\n",
        "df1510 = combined_df[combined_df['taken']>0]\n",
        "\n",
        "#creating 1511s sharing report\n",
        "\n",
        "combined_df = pd.DataFrame()\n",
        "\n",
        "for part in matches1511['Part'].unique():\n",
        "    # Filter the data frame for the current part and matches1501\n",
        "    part_df = matches1511[(matches1511['Part'] == part)]\n",
        "    # Define the new columns\n",
        "    taken_col = []\n",
        "    leftover_col = []\n",
        "    part_df = part_df.reset_index(drop=True)\n",
        "    \n",
        "    # Loop over each row in the filtered data frame\n",
        "    for i, row in part_df.iterrows():\n",
        "        if i == 0:\n",
        "            # For the first row, calculate the 'taken' and 'leftover' values\n",
        "            if row['Excess Qty_18 Wk'] > row['Trans QTY']:\n",
        "                taken = row['Trans QTY']\n",
        "            else:\n",
        "                taken = row['Excess Qty_18 Wk']\n",
        "            taken_col.append(taken)\n",
        "            leftover = row['Excess Qty_18 Wk'] - taken\n",
        "            leftover_col.append(leftover)\n",
        "        else:\n",
        "            # For all following rows, calculate the 'taken' and 'leftover' values based on the previous row's 'leftover' value\n",
        "            prev_leftover = leftover_col[-1]\n",
        "            if prev_leftover > row['Trans QTY']:\n",
        "                taken = row['Trans QTY']\n",
        "            else:\n",
        "                taken = prev_leftover\n",
        "            taken_col.append(taken)\n",
        "            leftover = prev_leftover - taken\n",
        "            leftover_col.append(leftover)\n",
        "    # Add the new columns to the filtered data frame\n",
        "    part_df['taken'] = taken_col\n",
        "    part_df['leftover'] = leftover_col\n",
        "    combined_df = pd.concat([combined_df, part_df], ignore_index=True)\n",
        "df1511 = combined_df[combined_df['taken']>0]\n",
        "\n",
        "#creating 1512s sharing report\n",
        "\n",
        "combined_df = pd.DataFrame()\n",
        "\n",
        "for part in matches1512['Part'].unique():\n",
        "    # Filter the data frame for the current part and matches1501\n",
        "    part_df = matches1512[(matches1512['Part'] == part)]\n",
        "    # Define the new columns\n",
        "    taken_col = []\n",
        "    leftover_col = []\n",
        "    part_df = part_df.reset_index(drop=True)\n",
        "    \n",
        "    # Loop over each row in the filtered data frame\n",
        "    for i, row in part_df.iterrows():\n",
        "        if i == 0:\n",
        "            # For the first row, calculate the 'taken' and 'leftover' values\n",
        "            if row['Excess Qty_18 Wk'] > row['Trans QTY']:\n",
        "                taken = row['Trans QTY']\n",
        "            else:\n",
        "                taken = row['Excess Qty_18 Wk']\n",
        "            taken_col.append(taken)\n",
        "            leftover = row['Excess Qty_18 Wk'] - taken\n",
        "            leftover_col.append(leftover)\n",
        "        else:\n",
        "            # For all following rows, calculate the 'taken' and 'leftover' values based on the previous row's 'leftover' value\n",
        "            prev_leftover = leftover_col[-1]\n",
        "            if prev_leftover > row['Trans QTY']:\n",
        "                taken = row['Trans QTY']\n",
        "            else:\n",
        "                taken = prev_leftover\n",
        "            taken_col.append(taken)\n",
        "            leftover = prev_leftover - taken\n",
        "            leftover_col.append(leftover)\n",
        "    # Add the new columns to the filtered data frame\n",
        "    part_df['taken'] = taken_col\n",
        "    part_df['leftover'] = leftover_col\n",
        "    combined_df = pd.concat([combined_df, part_df], ignore_index=True)\n",
        "df1512 = combined_df[combined_df['taken']>0]\n",
        "\n",
        "#creating 1601s sharing report\n",
        "\n",
        "combined_df = pd.DataFrame()\n",
        "\n",
        "for part in matches1601['Part'].unique():\n",
        "    # Filter the data frame for the current part and matches1501\n",
        "    part_df = matches1601[(matches1601['Part'] == part)]\n",
        "    # Define the new columns\n",
        "    taken_col = []\n",
        "    leftover_col = []\n",
        "    part_df = part_df.reset_index(drop=True)\n",
        "    \n",
        "    # Loop over each row in the filtered data frame\n",
        "    for i, row in part_df.iterrows():\n",
        "        if i == 0:\n",
        "            # For the first row, calculate the 'taken' and 'leftover' values\n",
        "            if row['Excess Qty_18 Wk'] > row['Trans QTY']:\n",
        "                taken = row['Trans QTY']\n",
        "            else:\n",
        "                taken = row['Excess Qty_18 Wk']\n",
        "            taken_col.append(taken)\n",
        "            leftover = row['Excess Qty_18 Wk'] - taken\n",
        "            leftover_col.append(leftover)\n",
        "        else:\n",
        "            # For all following rows, calculate the 'taken' and 'leftover' values based on the previous row's 'leftover' value\n",
        "            prev_leftover = leftover_col[-1]\n",
        "            if prev_leftover > row['Trans QTY']:\n",
        "                taken = row['Trans QTY']\n",
        "            else:\n",
        "                taken = prev_leftover\n",
        "            taken_col.append(taken)\n",
        "            leftover = prev_leftover - taken\n",
        "            leftover_col.append(leftover)\n",
        "    # Add the new columns to the filtered data frame\n",
        "    part_df['taken'] = taken_col\n",
        "    part_df['leftover'] = leftover_col\n",
        "    combined_df = pd.concat([combined_df, part_df], ignore_index=True)\n",
        "df1601 = combined_df[combined_df['taken']>0]\n",
        "\n",
        "#creating 1602s sharing report\n",
        "\n",
        "combined_df = pd.DataFrame()\n",
        "\n",
        "for part in matches1602['Part'].unique():\n",
        "    # Filter the data frame for the current part and matches1501\n",
        "    part_df = matches1602[(matches1602['Part'] == part)]\n",
        "    # Define the new columns\n",
        "    taken_col = []\n",
        "    leftover_col = []\n",
        "    part_df = part_df.reset_index(drop=True)\n",
        "    \n",
        "    # Loop over each row in the filtered data frame\n",
        "    for i, row in part_df.iterrows():\n",
        "        if i == 0:\n",
        "            # For the first row, calculate the 'taken' and 'leftover' values\n",
        "            if row['Excess Qty_18 Wk'] > row['Trans QTY']:\n",
        "                taken = row['Trans QTY']\n",
        "            else:\n",
        "                taken = row['Excess Qty_18 Wk']\n",
        "            taken_col.append(taken)\n",
        "            leftover = row['Excess Qty_18 Wk'] - taken\n",
        "            leftover_col.append(leftover)\n",
        "        else:\n",
        "            # For all following rows, calculate the 'taken' and 'leftover' values based on the previous row's 'leftover' value\n",
        "            prev_leftover = leftover_col[-1]\n",
        "            if prev_leftover > row['Trans QTY']:\n",
        "                taken = row['Trans QTY']\n",
        "            else:\n",
        "                taken = prev_leftover\n",
        "            taken_col.append(taken)\n",
        "            leftover = prev_leftover - taken\n",
        "            leftover_col.append(leftover)\n",
        "    # Add the new columns to the filtered data frame\n",
        "    part_df['taken'] = taken_col\n",
        "    part_df['leftover'] = leftover_col\n",
        "    combined_df = pd.concat([combined_df, part_df], ignore_index=True)\n",
        "df1602 = combined_df[combined_df['taken']>0]"
      ],
      "metadata": {
        "id": "DQF5tf5D9_nk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Put all the data frames into a list\n",
        "dfs = [df1501, df1502, df1503, df1504, df1505, df1506, df1507, df1508, df1509, df1510, df1511, df1512, df1601, df1602]\n",
        "\n",
        "# Concatenate the data frames along the rows (axis=0)\n",
        "rawsharing = pd.concat(dfs, axis=0)\n",
        "\n",
        "# Reset the index of the resulting data frame\n",
        "rawsharing = rawsharing.reset_index(drop=True)"
      ],
      "metadata": {
        "id": "LkGliZ5QT6nG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# rename columns and clear requested value colun\n",
        "\n",
        "rawsharing = rawsharing.rename( columns = {'Branch Plant':'From Plant','Header Branch':'To Plant','taken':'Requested Qty','leftover':'Proposed Value','Request Date':'MRP Demand Date'})\n",
        "rawsharing['Proposed Value'] = \"\"\n",
        "\n",
        "# multiply to find requested value\n",
        "\n",
        "rawsharing['Proposed Value'] = rawsharing['Requested Qty'] * rawsharing['Standard Cost']\n",
        "\n",
        "# drop any values where requested value is less than 100\n",
        "\n",
        "rawsharing = rawsharing[rawsharing['Proposed Value'] >= 100]\n",
        "\n",
        "# set column order\n",
        "\n",
        "cols = ['Part','Part Description','From Plant','Requested Qty','Standard Cost','To Plant','MRP Demand Date','Proposed Value']\n",
        "\n",
        "sharing = rawsharing[cols]\n",
        "\n",
        "# set part to string to preserve 0s when exporting the CSV\n",
        "\n",
        "sharing['Part'] = sharing['Part'].astype('|S')\n",
        "\n",
        "# write the sharing to csv\n",
        "\n",
        "sharing.to_csv('sharing.csv', index = False)\n",
        "files.download('sharing.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "id": "d4kCnY9kU0n_",
        "outputId": "4b2df6f7-986e-4786-d832-025831284729"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-22-a4fba8b6c4c0>:22: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  sharing['Part'] = sharing['Part'].astype('|S')\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_15946110-8765-4732-b6e1-8d89656fff4e\", \"sharing.csv\", 65400)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sharing.dtypes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k6kcGaaT8xPY",
        "outputId": "d5e091eb-d8a4-4b52-8347-504ffe60fb64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Part                          |S15\n",
              "Part Description            object\n",
              "From Plant                   int64\n",
              "Requested Qty              float64\n",
              "Standard Cost              float64\n",
              "To Plant                     int64\n",
              "MRP Demand Date     datetime64[ns]\n",
              "Proposed Value             float64\n",
              "dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    }
  ]
}